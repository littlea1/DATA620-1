{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment: Document Classification\n",
    "##### Data 620, \n",
    "###### Team 1: Jason Givens-Doyle, Mehdi Khan, Paul Britton\n",
    "\n",
    "Video link: https://youtu.be/Z_9_WaXEx34\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The assignment:\n",
    "It can be useful to be able to classify new \"test\" documents using already classified \"training\" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  Here is one example of such data:  http://archive.ics.uci.edu/ml/datasets/Spambase\n",
    "\n",
    "For this project, you can either use the above dataset to predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).\n",
    "\n",
    "For more adventurous students, you are welcome (encouraged!) to come up a different set of documents (including scraped web pages!?) that have already been classified (e.g. tagged), then analyze these documents to predict how new documents should be classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution:\n",
    "The Data: The project used data downloaded from  Dataturks (https://dataturks.com/) that were collected to  explore cyber-bullying through email, messaging etc. The dataset has 20001 texts or messages, each of which were tagged by '1' or '0' indicating offensive or normal messages respectively. The downloaded document was a text file with each line representing a text/message in json (python dictionary) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,  TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each line was in json format, json.loads were used to get the data in python environment. Then a dataframe was created from the data with two columns containing the messages and the tags (Label):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Get fucking real dude.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She is as dirty as they come  and that crook R...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why did you fuck it up. I could do it all day ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dude they dont finish enclosing the fucking sh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WTF are you talking about Men? No men thats no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content Label\n",
       "0                             Get fucking real dude.     1\n",
       "1  She is as dirty as they come  and that crook R...     1\n",
       "2  why did you fuck it up. I could do it all day ...     1\n",
       "3  Dude they dont finish enclosing the fucking sh...     1\n",
       "4  WTF are you talking about Men? No men thats no...     1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Cyber-Trolls.txt', 'r')as f:\n",
    "        dataset = [line.strip() for line in f]\n",
    "   \n",
    "d = [json.loads(dictobject) for dictobject in dataset]\n",
    "df = pd.DataFrame(d)\n",
    "df.drop([\"extras\",\"metadata\"], axis=1, inplace=True)\n",
    "lbl= [d.get('label')[0] for d in df[df.columns[0]]]\n",
    "df['Label']=lbl\n",
    "df.drop([\"annotation\"], inplace=True,axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12179</td>\n",
       "      <td>11853</td>\n",
       "      <td>nope</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7822</td>\n",
       "      <td>2789</td>\n",
       "      <td>#NAME?</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      content                    \n",
       "        count unique     top freq\n",
       "Label                            \n",
       "0       12179  11853    nope   21\n",
       "1        7822   2789  #NAME?   18"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table shows that there are 12,179 messages that were not considered offensive while 7,822 messages were offensive. The most frequent words in both categories do not seem to carry any special meaning toward those two categories. One interesting finding is that there are lesser number of unique words in the offensive messages indicating that same words were used to offend people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avg. length of normal text:</td>\n",
       "      <td>64.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avg. length of offensive text:</td>\n",
       "      <td>70.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        \n",
       "0     Avg. length of normal text:  64.96\n",
       "1  Avg. length of offensive text:  70.78"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.DataFrame([['Avg. length of normal text:',\n",
    "                       round(df.loc[df.Label=='0','content'].apply(len).mean(),2)],\n",
    "                      ['Avg. length of offensive text:',\n",
    "                       round(df.loc[df.Label=='1','content'].apply(len).mean(),2)]])\n",
    "table.columns=['']*len(table.columns)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above table suggests average length of offensive messages is a bit higher than that of normal messages, so message length may not be an indicator of offensive or non offensive texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Pre- processing \n",
    "All the messages were tokenized i.e. each of the messages were split into individual words i.e. each of the texts were represented as a set of words. The punctuation were also removed from each of texts. Regular expression and tokenize from nltk library were used on the content column of the dataframe to achieve both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Get, fucking, real, dude]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[She, is, as, dirty, as, they, come, and, that...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[why, did, you, fuck, it, up, I, could, do, it...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Dude, they, dont, finish, enclosing, the, fuc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[WTF, are, you, talking, about, Men, No, men, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content Label\n",
       "0                         [Get, fucking, real, dude]     1\n",
       "1  [She, is, as, dirty, as, they, come, and, that...     1\n",
       "2  [why, did, you, fuck, it, up, I, could, do, it...     1\n",
       "3  [Dude, they, dont, finish, enclosing, the, fuc...     1\n",
       "4  [WTF, are, you, talking, about, Men, No, men, ...     1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['content']=df.content.apply(tokenizer.tokenize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common words such as 'the', 'a', 'are' etc. that are not useful but occur frequently were also removed using stopwards from nltk package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Get, fucking, real, dude]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[dirty, come, crook, Rengel, Dems, fucking, co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[fuck, could, day, Let, hour, Ping, later, sch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Dude, dont, finish, enclosing, fucking, showe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[WTF, talking, Men, men, thats, menage, gay]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content Label\n",
       "0                         [Get, fucking, real, dude]     1\n",
       "1  [dirty, come, crook, Rengel, Dems, fucking, co...     1\n",
       "2  [fuck, could, day, Let, hour, Ping, later, sch...     1\n",
       "3  [Dude, dont, finish, enclosing, fucking, showe...     1\n",
       "4       [WTF, talking, Men, men, thats, menage, gay]     1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(txt):\n",
    "    removed = [word for word in txt if word.lower() not in stopwords.words('english')] \n",
    "    return removed\n",
    "\n",
    "df['content']=df.content.apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization\n",
    "Vectorization is a process that converts text into numbers. So far each of the texts (messages) were converted into a list of tokens (words). The vectorization is needed to represent these texts in ML algorithms. There are multiple ways to do vectorization and some combination of which will be used here:\n",
    "\n",
    "1. Term Document Matrix or term frequency: Counting the number of times each word appears in a document\n",
    "2. TF-IDF (Term Frequency-Inverse Document Frequency): Weighting the frequency to asses relative importance of a term in the document and the entire corpus\n",
    "\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "TF-IDF Vectors can be generated at words, characters, or n-grams levels. Only word level will be applied here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before vectorization was done, two sets of data were created to train and test classification models, the rows were selected randomly so that both datasets have reasonable numbers of offensive and non offensive messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = [\" \".join(con) for con in df['content'].values]\n",
    "df_train = df.sample(frac=.8, random_state=100).reset_index(drop=True)\n",
    "df_test = df.drop(df_train.index).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anyone said Arial gonna beat ass</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hahaha Well take care would hate lose homeland...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipina foreigner</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hope singing along hate think torturing poor l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Duck tape fixes everything</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>says gubgivits teeth thingies stole blood ever...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sucks</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>N3qRO WUZZ YA NUMBER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content Label\n",
       "0                   anyone said Arial gonna beat ass     1\n",
       "1  Hahaha Well take care would hate lose homeland...     1\n",
       "2                                 filipina foreigner     0\n",
       "3  hope singing along hate think torturing poor l...     1\n",
       "4                         Duck tape fixes everything     0\n",
       "5  says gubgivits teeth thingies stole blood ever...     1\n",
       "6                                              sucks     1\n",
       "7                               N3qRO WUZZ YA NUMBER     0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>okay seen long time</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OKay Hope everything goes okay</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>would found 100 movie theater</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wow really persistent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yuh stalkers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          content Label\n",
       "0             okay seen long time     0\n",
       "1  OKay Hope everything goes okay     0\n",
       "2   would found 100 movie theater     0\n",
       "3           Wow really persistent     0\n",
       "4                    yuh stalkers     0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train data prepration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = CountVectorizer()\n",
    "# ignore terms that appear in more than 50% of the documents\n",
    "#bag_of_words.set_params(max_df=0.5)\n",
    "bag_of_words_fit = bag_of_words.fit(df_train['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1792)\t2\n",
      "  (0, 4575)\t1\n",
      "  (0, 5873)\t1\n",
      "  (0, 8744)\t1\n",
      "  (0, 11513)\t1\n",
      "  (0, 12052)\t1\n",
      "  (0, 12634)\t1\n",
      "  (0, 12691)\t1\n",
      "  (0, 12848)\t1\n",
      "  (0, 13196)\t1\n",
      "  (0, 13364)\t1\n"
     ]
    }
   ],
   "source": [
    "tdm_train = bag_of_words_fit.transform(df_train['content'])\n",
    "print(tdm_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term weighting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16001, 15182)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_fit_train = TfidfTransformer().fit(tdm_train)\n",
    "tfidf_train = tfidf_fit_train.transform(tdm_train)\n",
    "tfidf_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16001x15182 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 109406 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test data prepration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 15182)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bag_of_words_test = CountVectorizer()\n",
    "# ignore terms that appear in more than 50% of the documents\n",
    "#bag_of_words_test.set_params(max_df=0.5)\n",
    "#bag_of_words_test.fit(df_test['content'])\n",
    "\n",
    "tdm_test =bag_of_words.transform(df_test['content'])\n",
    "tdm_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4804)\t1\n",
      "  (0, 5968)\t1\n",
      "  (0, 6725)\t1\n"
     ]
    }
   ],
   "source": [
    "print(tdm_test[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 15182)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_fit_test = TfidfTransformer().fit(tdm_test)\n",
    "tfidf_test = tfidf_fit_test.transform(tdm_test)\n",
    "tfidf_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4000x15182 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 18540 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building:\n",
    "\n",
    "Next we examine various different classifier models in the context of this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Naive Bayes Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "online_troll_model_NB = MultinomialNB().fit(tfidf_train,df_train['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediting Label for test dataset using the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted_label = online_troll_model_NB.predict(tfidf_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98775"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(df_test['Label'], predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive bayes model showed almost 99% accuracy, which is impressive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Forest Model:\n",
    "\n",
    "Here we explore the train_test_split function in sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.8386935510748209\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "tfidf =  bag_of_words.fit_transform(df['content'])\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(tfidf,df['Label'],test_size=0.3,random_state=42)\n",
    "\n",
    "online_troll_model_RF = RandomForestClassifier().fit(X_train,y_train)\n",
    "predicted= online_troll_model_RF.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest model substantially under-performs the naive bayes test\n",
    "\n",
    "###### K-Nearest-Neighbors Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 0.6978836860523246\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "tfidf =  bag_of_words.fit_transform(df['content'])\n",
    "\n",
    "online_troll_model_KNN = KNeighborsClassifier().fit(X_train,y_train)\n",
    "predicted= online_troll_model_KNN.predict(X_test)\n",
    "print(\"KNN Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN model substantially underperforms both the Naive Bayes and Random Forest models\n",
    "\n",
    "###### SVM  Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.6108981836360606\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "online_troll_model_SVM = SVC().fit(X_train,y_train)\n",
    "predicted= online_troll_model_SVM.predict(X_test)\n",
    "print(\"SVM Accuracy:\",metrics.accuracy_score(y_test, predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the SVM model substantially underperforms the first model attempted\n",
    "\n",
    "###### AdaBoost  Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 0.6868855190801533\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "online_troll_model_ADA = AdaBoostClassifier().fit(X_train,y_train)\n",
    "predicted= online_troll_model_ADA.predict(X_test)\n",
    "print(\"AdaBoost Accuracy:\",metrics.accuracy_score(y_test, predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adaboost model also underperforms the  Naive Bayes as well\n",
    "\n",
    "###### GradientBoost  Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.6922179636727213\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "online_troll_model_GBC = GradientBoostingClassifier().fit(X_train,y_train)\n",
    "predicted= online_troll_model_GBC.predict(X_test)\n",
    "print(\"SVM Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the Gradient Boost model is yet another underperformer.  At this point, a baysian thinker would probably conclude that the naive bayes approach is an effective approach for this problem.\n",
    "\n",
    "\n",
    "##### Ensemble:\n",
    "\n",
    "Finally, we examine an ensemble of all the models tested above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77 [Naive Bayes]\n",
      "Accuracy: 0.83 [Random Forests]\n",
      "Accuracy: 0.70 [KNN]\n",
      "Accuracy: 0.61 [SVM]\n",
      "Accuracy: 0.70 [AdaBoost]\n",
      "Accuracy: 0.71 [GradientBoost]\n",
      "Accuracy: 0.73 [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore')  #suppressing a few warnings... \n",
    "\n",
    "#create the models\n",
    "NB = MultinomialNB()\n",
    "RF = RandomForestClassifier()\n",
    "KNN = KNeighborsClassifier()\n",
    "SVM = SVC()\n",
    "ADA = AdaBoostClassifier()\n",
    "GBC = GradientBoostingClassifier()\n",
    "\n",
    "\n",
    "clf = [NB,RF,KNN,SVM,ADA,GBC]\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('Naive Bayes', NB),\n",
    "                                    ('Random Forests', RF),\n",
    "                                    ('KNN', KNN),\n",
    "                                    ('SVM', SVM),\n",
    "                                    ('AdaBoost', ADA),\n",
    "                                    ('GradientBoost', GBC)], voting='hard')\n",
    "\n",
    "scoreDict = {}\n",
    "\n",
    "for clf, label in zip([NB,RF,KNN,SVM,ADA,GBC,eclf], ['Naive Bayes', 'Random Forests','KNN','SVM','AdaBoost','GradientBoost', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    scoreDict[label] = scores.mean()\n",
    "    print(\"Accuracy: %0.2f [%s]\" % (scores.mean(), label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(range(len(scoreDict)), list(scoreDict.values()), align='center')\n",
    "plt.xticks(range(len(scoreDict)), list(scoreDict.keys()),rotation='vertical')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above that the ensemble doesn't add much value here.  In fact, we might want to drop a few of these models altogether in order to try to improve performance.  Next steps here would be to run a GridSearchCV in order to tune the collection of models found here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
