{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used here is from [Apache SpamAssassin](data sourcehttps://spamassassin.apache.org/old/publiccorpus/) and contains a few thousand classified \"spam\" and \"ham\" emails.\n",
    "\n",
    "The uncompressed data is a bit large for github, so we've preprocessed the data and stored it in a single CSV file [here](https://raw.githubusercontent.com/plb2018/data620/master/Week10/spam_n_ham.csv) to make life easier.  The compressed data has been archived [here](https://github.com/plb2018/data620/tree/master/Week10/raw_data) for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH = \"C:/Users/Paul/OneDrive - CUNY School of Professional Studies/CUNY/DATA 620/Week10/\"\n",
    "spam_folders = ['easy_ham/','spam/','easy_ham_2/','hard_ham/','spam_2/']\n",
    "\n",
    "cols = ['body','source','isSpam']\n",
    "df = pd.DataFrame(columns = cols)\n",
    "\n",
    "for folder in spam_folders:\n",
    "    files = [f for f in listdir(PATH+folder) if isfile(join(PATH+folder, f))]\n",
    "    \n",
    "    for f in files:\n",
    "        data = open(PATH+folder+f,'rb')\n",
    "          \n",
    "        newData = {'body':data.read(),\n",
    "                  'source':folder,\n",
    "                  'isSpam':0}\n",
    "        \n",
    "        df = df.append(newData,ignore_index=True)\n",
    "        data.close()\n",
    "\n",
    "#encode the spam as spam\n",
    "df['isSpam'][(df['source'] == 'spam/') | (df['source'] == 'spam_2/')] = 1   \n",
    "\n",
    "#df.to_csv(\"spam_n_ham.csv\",sep='|')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9348, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " def cleanData(data):\n",
    "    data=str(data)\n",
    "    data = data.lower()\n",
    "    data=data.replace('{html}',\"\") \n",
    "    clnr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(clnr, '', data)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body']=df['body'].map(lambda s:cleanData(s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/plb2018/data620/master/Week10/spam_n_ham.csv\",sep='|',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tts_idx = np.random.rand(df.shape[0]) < 0.75\n",
    "\n",
    "train = df[tts_idx].sample(frac=1) \n",
    "test = df[~tts_idx].sample(frac=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isSpam.sum() /df.shape[0]\n",
    "train.isSpam.sum()/train.shape[0]\n",
    "test.isSpam.sum()/test.shape[0]\n",
    "\n",
    "test.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spam = ' '.join(list(df[df['isSpam'] == 1]['body']))\n",
    "#spam_wc = WordCloud(width = 512, height=512).generate(spam)\n",
    "#plt.figure(figsize = (10,8), facecolor = 'w')\n",
    "#plt.imshow(spam_wc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_finder_tfidf = SpamClassifier(train,'tf-idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
